{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Milestone I Natural Language Processing\n",
    "## Task 1. Basic Text Pre-processing\n",
    "#### Student Name: Amay Viswanathan Iyer\n",
    "#### Student ID: 3970066\n",
    "\n",
    "Date: October 6, 2023\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Environment: Python 3 and Jupyter notebook\n",
    "\n",
    "Libraries used: please include all the libraries you used in your assignment, e.g.,:\n",
    "* pandas\n",
    "* re\n",
    "* numpy\n",
    "* random\n",
    "\n",
    "## Introduction\n",
    "In Task 1 I extracted the data from the folder and tokenized it based on the specifications provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here, I have written code to import the libraries needed for this assessment, e.g., numpy and pandas\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# For NLP tasks, i imported the Regular Expressions Tokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Examining and loading data\n",
    "- xamine the data folder, including the categories and job advertisment txt documents, etc. Explain your findings here, e.g., number of folders and format of txt files, etc.\n",
    "- Load the data into proper data structures and get it ready for processing.\n",
    "- Extract webIndex and description into proper data structures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Category     Job_ID                                            Content\n",
      "0    Sales  Job_00776  Title: Estate Agency Senior Sales Negotiator\\n...\n",
      "1    Sales  Job_00762  Title: Export Sales Executive (French & German...\n",
      "2    Sales  Job_00763  Title: GRADUATE SALES ENGINEER\\nWebindex: 6825...\n",
      "3    Sales  Job_00749  Title: Sales Representative / Lead Generator\\n...\n",
      "4    Sales  Job_00761  Title: Search Recruitment Consultant  Media an...\n"
     ]
    }
   ],
   "source": [
    "# Code to inspect the provided data file...\n",
    "# Defining a function to extract data from each job advertisement\n",
    "def extract_data_from_folder(base_path):\n",
    "    data = []\n",
    "    \n",
    "    # Listing all category folders\n",
    "    categories = os.listdir(base_path)\n",
    "    \n",
    "    for category in categories:\n",
    "        category_path = os.path.join(base_path, category)\n",
    "        \n",
    "        # Making sure I'm looking only at folders and not stray files\n",
    "        if os.path.isdir(category_path):\n",
    "            files = os.listdir(category_path)\n",
    "            \n",
    "            for file in files:\n",
    "                file_path = os.path.join(category_path, file)\n",
    "                \n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    # Typically, the description is the entire content after the webindex\n",
    "                    content = f.read()\n",
    "                    # A simple regex split could help segregate title, webindex, and description, \n",
    "                    #but here I'll consider the entire content for simplicity and later tokenize it using \n",
    "                    #the regex in subsequent cells\n",
    "                    data.append([category, file.split('.')[0], content])\n",
    "                    \n",
    "    return pd.DataFrame(data, columns=['Category', 'Job_ID', 'Content'])\n",
    "\n",
    "# Assuming the base path is 'data' \n",
    "base_path = 'data'\n",
    "df = extract_data_from_folder(base_path)\n",
    "\n",
    "# displaying the first few rows of the dataframe\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Pre-processing data\n",
    "Perform the required text pre-processing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...... Sections and code blocks on basic text pre-processing\n",
    "\n",
    "\n",
    "<span style=\"color: red\"> You might have complex notebook structure in this section, please feel free to create your own notebook structure. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to perform the task with the tokenizer\n",
    "tokenizer = RegexpTokenizer(r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\")\n",
    "\n",
    "def tokenize(text):\n",
    "    return tokenizer.tokenize(text.lower())\n",
    "\n",
    "df['Tokens'] = df['Content'].apply(tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing small words\n",
    "def remove_small_words(tokens):\n",
    "    return [token for token in tokens if len(token) > 1]\n",
    "\n",
    "df['Tokens'] = df['Tokens'].apply(remove_small_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the stopwords file \n",
    "with open(\"stopwords_en.txt\", \"r\") as file:\n",
    "    stopwords = file.read().splitlines()\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    return [token for token in tokens if token not in stopwords]\n",
    "\n",
    "df['Tokens'] = df['Tokens'].apply(remove_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the rare words as well\n",
    "all_tokens = [token for sublist in df['Tokens'].tolist() for token in sublist]\n",
    "freq = pd.Series(all_tokens).value_counts()\n",
    "\n",
    "def remove_rare_words(tokens):\n",
    "    return [token for token in tokens if freq[token] > 1]\n",
    "\n",
    "df['Tokens'] = df['Tokens'].apply(remove_rare_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "top_50 = freq.head(50).index.tolist()\n",
    "\n",
    "def remove_top_frequent(tokens):\n",
    "    return [token for token in tokens if token not in top_50]\n",
    "\n",
    "df['Tokens'] = df['Tokens'].apply(remove_top_frequent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Processed_Content'] = df['Tokens'].apply(lambda x: ' '.join(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing all the work in vocab.txt as required\n",
    "vocabulary = sorted(list(set([token for sublist in df['Tokens'].tolist() for token in sublist])))\n",
    "vocab_dict = {word: index for index, word in enumerate(vocabulary)}\n",
    "\n",
    "with open(\"vocab.txt\", \"w\") as file:\n",
    "    for word, index in vocab_dict.items():\n",
    "        file.write(f\"{word}:{index}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "638    commercial catering laundry equipment northamp...\n",
      "555    security officer higher level carillion plc ov...\n",
      "286    reporting analyst pepsico qualified accountant...\n",
      "147    senior negotiator independent estate agency fa...\n",
      "188    associate director equity car ukstaffsearch as...\n",
      "Name: Processed_Content, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Inspecting random 5 rows in the Processed_Content Column again\n",
    "\n",
    "# Displaying 5 random rows from the Processed_Content column\n",
    "print(df['Processed_Content'].sample(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defining: 1292\n",
      "public: 3898\n",
      "contacted: 1075\n",
      "britain's: 609\n",
      "merit: 3036\n",
      "lane: 2717\n",
      "weekend: 5246\n",
      "attention: 382\n",
      "uk's: 5039\n",
      "addresses: 84\n"
     ]
    }
   ],
   "source": [
    "#inspecting the vocab.txt file\n",
    "import random\n",
    "\n",
    "# Extracting 10 random key-value pairs from the vocabulary\n",
    "random_vocab_entries = random.sample(list(vocab_dict.items()), 10)\n",
    "for word, index in random_vocab_entries:\n",
    "    print(f\"{word}: {index}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving required outputs\n",
    "Save the vocabulary, bigrams and job advertisment txt as per spectification.\n",
    "- vocab.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Give a short summary and anything you would like to talk about the assessment task here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Couple of notes for all code blocks in this notebook\n",
    "- please provide proper comment on your code\n",
    "- Please re-start and run all cells to make sure codes are runable and include your output in the submission.   \n",
    "<span style=\"color: red\"> This markdown block can be removed once the task is completed. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
